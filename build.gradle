/*
 * Copyright (c) 2017-2020 TIBCO Software Inc. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you
 * may not use this file except in compliance with the License. You
 * may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 * implied. See the License for the specific language governing
 * permissions and limitations under the License. See accompanying
 * LICENSE file.
 */

import org.gradle.api.tasks.testing.logging.*

apply plugin: 'wrapper'

// TODO: profiles and allow changing hadoopVersion

buildscript {
  repositories {
    maven { url 'https://plugins.gradle.org/m2' }
    mavenCentral()
  }
  dependencies {
    classpath 'io.snappydata:gradle-scalatest:0.25'
    classpath 'org.github.ngbinh.scalastyle:gradle-scalastyle-plugin_2.11:1.0.1'
    classpath 'com.github.jengelman.gradle.plugins:shadow:5.2.0'
    classpath 'com.commercehub.gradle.plugin:gradle-avro-plugin:0.9.1'
  }
}

description = 'Spark Project'

allprojects {
  // We want to see all test results.  This is equivalent to setting --continue
  // on the command line.
  gradle.startParameter.continueOnFailure = true

  repositories {
    mavenCentral()
    maven { url 'https://repo.hortonworks.com/content/repositories/releases' }
  }

  apply plugin: 'java'
  apply plugin: 'com.github.johnrengelman.shadow'
  apply plugin: 'idea'

  group = 'io.snappydata'
  version = snappySparkVersion

  ext {
    vendorName = 'TIBCO Software Inc.'
    scalaBinaryVersion = '2.11'
    scalaVersion = scalaBinaryVersion + '.12'
    scalaParserVersion = '1.1.2'
    hadoopVersion = '2.7.7'
    protobufVersion = '3.6.1'
    jerseyVersion = '2.22.2'
    sunJerseyVersion = '1.19.4'
    jettyVersion = '9.3.28.v20191105'
    log4jVersion = '1.2.17'
    slf4jVersion = '1.7.30'
    junitVersion = '4.12'
    mockitoVersion = '1.10.19'
    jmockVersion = '2.8.4'
    javaxServletVersion = '3.1.0'
    guavaVersion = '14.0.1'
    hiveVersion = '1.21.2.3.1.2.0-4'
    chillVersion = '0.9.3'
    kryoVersion = '4.0.2'
    nettyVersion = '3.10.6.Final'
    nettyAllVersion = '4.1.45.Final'
    derbyVersion = '10.14.2.0'
    httpClientVersion = '4.5.11'
    httpCoreVersion = '4.4.13'
    levelDbJniVersion = '1.8'
    jackson1Version = '1.9.13'
    jacksonVersion = '2.9.9'
    jacksonBindVersion = '2.9.9'
    snappyJavaVersion = '1.1.7.3'
    lz4Version = '1.4.0'
    lzfVersion = '1.0.4'
    zstdVersion = '1.3.2-2'
    parquetVersion = '1.10.1'
    orcVersion = '1.5.8'
    metricsVersion = '3.2.6'
    janinoVersion = '3.0.9'
    thriftVersion = '0.9.3'
    antlrVersion = '4.7.2'
    jpamVersion = '1.1'
    seleniumVersion = '2.52.0'
    curatorVersion = '2.7.1'
    commonsCodecVersion = '1.11'
    commonsCryptoVersion = '1.0.0'
    arrowVersion = '0.10.0'
    commonsIoVersion = '2.6'
    commonsLang3Version = '3.8.1'
    commonsMath3Version = '3.6.1'
    commonsNetVersion = '3.6'
    paranamerVersion = '2.8'
    avroVersion = '1.8.2'
    jsr305Version = '3.0.2'
    jlineVersion = '2.14.6'
    xbeanAsm6Version = '4.10'
    breezeVersion = '0.13.2'
    pmmlVersion = '1.2.17'
    classutilVersion = '1.4.0'
    scoptVersion = '3.7.1'
    mesosVersion = '1.4.3'
    k8sVersion = '4.6.1'
    netlibVersion = '1.1.2'
    arpackVersion = '0.1'
    kafka2Version = '2.0.1'
    joptVersion = '5.0.4'
    flumeVersion = '1.8.0'
    db2JdbcVersion = '10.5.0.5'
    dockerClientVersion = '8.14.5'
    mysqlVersion = '8.0.13'
    postgresqlVersion = '42.2.5'
    ojdbc6Version = '11.2.0.1.0'
    zookeeperVersion = '3.4.14'
    activationVersion = '1.1.1'
    roaringBitmapVersion = '0.7.45'
    json4sVersion = '3.5.5'
    streamVersion = '2.9.6'
    ivyVersion = '2.4.0'
    oroVersion = '2.0.8'
    pyroliteVersion = '4.22'
    py4jVersion = '0.10.9'
    xmlApisVersion = '1.4.01'
    datanucleusCoreVersion = '3.2.15'
    datanucleusJdoVersion = '3.2.8'
    datanucleusRdbmsVersion = '3.2.13'
    calciteVersion = '1.4.0-incubating'
    jodaTimeVersion = '2.10.1'
    joddVersion = '3.9.1'
    univocityVersion = '2.7.6'
    h2Version = '1.4.195'
    jettyJspVersion = '2.2.0.v201112011158'
    jettyJstlVersion = '1.2.0.v201105211821'

    scalatestVersion = '3.0.8'
    scalaCheckVersion = '1.13.5'
    junitInterfaceVersion = '0.11'
    hamcrestVersion = '1.3'
    pegdownVersion = '1.6.0'

    shadePackageName = 'org.spark_project'
  }

  // default output directory like in sbt/maven
  buildDir = 'build-artifacts/scala-' + scalaBinaryVersion

  ext {
    if (rootProject.name == 'snappy-spark') {
      subprojectBase = ':'
      sparkProjectRoot = ':'
      sparkProjectRootDir = project(':').projectDir
      testResultsBase = "${rootProject.buildDir}/tests"
      gitCmd = "git --git-dir=${rootDir}/.git --work-tree=${rootDir}"
    } else {
      subprojectBase = ':snappy-spark:'
      sparkProjectRoot = ':snappy-spark'
      sparkProjectRootDir = project(':snappy-spark').projectDir
      testResultsBase = "${rootProject.buildDir}/tests/spark"
      gitCmd = "git --git-dir=${project(sparkProjectRoot).projectDir}/.git --work-tree=${project(sparkProjectRoot).projectDir}"
    }
    snappyProductDir = "${rootProject.buildDir}/snappy"
  }
}

def getStackTrace(def t) {
  java.io.StringWriter sw = new java.io.StringWriter()
  java.io.PrintWriter pw = new java.io.PrintWriter(sw)
  org.codehaus.groovy.runtime.StackTraceUtils.sanitize(t).printStackTrace(pw)
  return sw.toString()
}

task cleanSparkScalaTest { doLast {
  def workingDir = "${testResultsBase}/scalatest"
  delete workingDir
  file(workingDir).mkdirs()
} }
task cleanSparkJUnit { doLast {
  def workingDir = "${testResultsBase}/junit"
  delete workingDir
  file(workingDir).mkdirs()
} }


subprojects {
  apply plugin: 'scala'
  apply plugin: 'maven'
  apply plugin: 'scalaStyle'

  // apply compiler options
  tasks.withType(JavaCompile) {
    options.encoding = 'UTF-8'
    options.incremental = true
    options.compilerArgs << '-Xlint:-serial,-path,-deprecation,-unchecked,-rawtypes,-try'
    options.compilerArgs << '-XDignore.symbol.file'
    options.fork = true
    options.forkOptions.javaHome = file(System.properties['java.home'])
    options.forkOptions.jvmArgs = [ '-J-Xmx2g', '-J-Xms2g', '-J-XX:ReservedCodeCacheSize=512m', '-J-Djava.net.preferIPv4Stack=true' ]
  }
  tasks.withType(ScalaCompile) {
    options.encoding = 'UTF-8'
    options.fork = true
    options.forkOptions.jvmArgs = [ '-Xmx2g', '-Xms2g', '-XX:ReservedCodeCacheSize=512m', '-Djava.net.preferIPv4Stack=true' ]
    // scalaCompileOptions.optimize = true
    // scalaCompileOptions.useAnt = false
    scalaCompileOptions.deprecation = false
    scalaCompileOptions.additionalParameters = [ '-feature', '-explaintypes', '-Yno-adapted-args' ]
  }

  javadoc.options.charSet = 'UTF-8'

  scalaStyle {
    configLocation = "${sparkProjectRootDir}/scalastyle-config.xml"
    inputEncoding = 'UTF-8'
    outputEncoding = 'UTF-8'
    outputFile = "${buildDir}/scalastyle-output.xml"
    includeTestSourceDirectory = false
    source = 'src/main/scala'
    testSource = 'src/test/scala'
    failOnViolation = true
    failOnWarning = false
  }

  configurations {
    runtimeJar {
      description 'a dependency to include additional jars at runtime'
      visible true
    }
  }

  // when invoking from snappydata, below are already defined at top-level
  if (rootProject.name == 'snappy-spark') {
    task packageSources(type: Jar, dependsOn: classes) {
      classifier = 'sources'
      from sourceSets.main.allSource
    }

    configurations {
      testOutput {
        extendsFrom testCompile
        description 'a dependency that exposes test artifacts'
      }
    }

    task packageTests(type: Jar, dependsOn: testClasses) {
      description 'Assembles a jar archive of test classes.'
      from sourceSets.test.output.classesDirs
      classifier = 'tests'
    }
    artifacts {
      testOutput packageTests
    }
  }
  task packageScalaDocs(type: Jar, dependsOn: scaladoc) {
    classifier = 'javadoc'
    from scaladoc
  }
  if (rootProject.hasProperty('enablePublish')) {
    artifacts {
      archives packageScalaDocs, packageSources
    }
  }

  // fix scala+java mix to all use compileScala which use correct dependency order
  sourceSets.main.scala.srcDir 'src/main/java'
  sourceSets.main.java.srcDirs = []

  dependencies {
    // This is a dummy dependency that is used along with the shading plug-in
    // to create effective poms on publishing (see SPARK-3812).
    //compile group: 'org.spark-project.spark', name: 'unused', version: '1.0.0'
    compile 'org.scala-lang:scala-library:' + scalaVersion
    compile 'org.scala-lang:scala-reflect:' + scalaVersion

    compile group: 'log4j', name:'log4j', version: log4jVersion
    compile 'org.slf4j:slf4j-api:' + slf4jVersion
    compile 'org.slf4j:slf4j-log4j12:' + slf4jVersion

    testCompile "junit:junit:${junitVersion}"
    testCompile "org.scalatest:scalatest_${scalaBinaryVersion}:${scalatestVersion}"
    testCompile "org.mockito:mockito-core:${mockitoVersion}"
    testCompile "org.scalacheck:scalacheck_${scalaBinaryVersion}:${scalaCheckVersion}"
    testCompile "com.novocode:junit-interface:${junitInterfaceVersion}"

    testRuntime "org.pegdown:pegdown:${pegdownVersion}"
  }

  if (rootProject.name == 'snappy-spark') {
    task scalaTest(type: Test) {
      actions = [ new com.github.maiflai.ScalaTestAction() ]

      testLogging.exceptionFormat = TestExceptionFormat.FULL
      testLogging.events = TestLogEvent.values() as Set

      extensions.add(com.github.maiflai.ScalaTestAction.TAGS, new org.gradle.api.tasks.util.PatternSet())
      List<String> suites = []
      extensions.add(com.github.maiflai.ScalaTestAction.SUITES, suites)
      extensions.add('suite', { String name -> suites.add(name) } )
      extensions.add('suites', { String... name -> suites.addAll(name) } )

      def result = new StringBuilder()
      extensions.add(com.github.maiflai.ScalaTestAction.TESTRESULT, result)
      extensions.add('testResult', { String name -> result.setLength(0); result.append(name) })

      def output = new StringBuilder()
      extensions.add(com.github.maiflai.ScalaTestAction.TESTOUTPUT, output)
      extensions.add('testOutput', { String name -> output.setLength(0); output.append(name) })

      def errorOutput = new StringBuilder()
      extensions.add(com.github.maiflai.ScalaTestAction.TESTERROR, errorOutput)
      extensions.add('testError', { String name -> errorOutput.setLength(0); errorOutput.append(name) })

      // running a single scala suite
      if (rootProject.hasProperty('singleSuite')) {
        suite singleSuite
      }
    }
  }
  scalaTest {
    // top-level default is single process run since scalatest does not
    // spawn separate JVMs
    maxParallelForks = 1
    systemProperties 'test.src.tables': '__not_used__'

    workingDir = "${testResultsBase}/scalatest"

    // testResult '/dev/tty'
    testOutput "${workingDir}/output.txt"
    testError "${workingDir}/error.txt"
    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  test {
    jvmArgs '-Xss4096k'
    maxParallelForks = Runtime.getRuntime().availableProcessors()
    systemProperties 'spark.master.rest.enabled': 'false',
      'test.src.tables': 'src'

    workingDir = "${testResultsBase}/junit"

    binResultsDir = file("${workingDir}/binary/${project.name}")
    reports.html.destination = file("${workingDir}/html/${project.name}")
    reports.junitXml.destination = file(workingDir)
  }
  test.dependsOn subprojectBase + 'cleanSparkJUnit'
  scalaTest.dependsOn subprojectBase + 'cleanSparkScalaTest'
  check.dependsOn scalaTest
  if (rootProject.name == 'snappy-spark') {
    check.dependsOn "${subprojectBase}snappy-spark-assembly_${scalaBinaryVersion}:sparkProduct"
  }
}

// need to do below after graph is ready else it will give an error about
// runtimeClaspath being set after being finalized
gradle.taskGraph.whenReady { graph ->
  def allTasks = subprojects.collect { it.tasks }.flatten()
  allTasks.each { task ->
    if (task instanceof Test) {
      def test = (Test)task
      test.configure {
        onlyIf { ! Boolean.getBoolean('skip.tests') }

        jvmArgs '-ea', '-XX:+HeapDumpOnOutOfMemoryError','-XX:+UseConcMarkSweepGC',
                '-XX:+UseParNewGC', '-XX:+CMSClassUnloadingEnabled',
                '-Xss4m', '-XX:ReservedCodeCacheSize=1g'
        minHeapSize '4g'
        maxHeapSize '4g'
        // disable assertions for hive tests as in Spark's pom.xml because HiveCompatibilitySuite currently fails (SPARK-4814)
        if (test.project.name.contains('snappy-spark-hive_')) {
          jvmArgs '-da'
          maxParallelForks = 1
        } else {
          jvmArgs '-ea'
        }
        environment 'SPARK_DIST_CLASSPATH': test.classpath.asPath,
          'SPARK_PREPEND_CLASSES': '1',
          'SPARK_SCALA_VERSION': scalaBinaryVersion,
          'SPARK_TESTING': '1',
          'JAVA_HOME': System.getProperty('java.home')
        systemProperties 'log4j.configuration': "file:${projectDir}/src/test/resources/log4j.properties",
          'derby.system.durability': 'test',
          'java.awt.headless': 'true',
          'java.io.tmpdir': "${rootProject.buildDir}/tmp",
          'spark.test.home': snappyProductDir,
          'spark.project.home': "${project(sparkProjectRoot).projectDir}",
          'spark.testing': '1',
          'spark.master.rest.enabled': 'false',
          'spark.ui.enabled': 'false',
          'spark.ui.showConsoleProgress': 'false',
          'spark.unsafe.exceptionOnMemoryLeak': 'true',
          'spark.memory.debugFill': 'true'

        testLogging.exceptionFormat = 'full'

        if (rootProject.name == 'snappy-spark') {
          def eol = System.getProperty('line.separator')
          beforeTest { desc ->
            def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
            def progress = new File(workingDir, 'progress.txt')
            def output = new File(workingDir, 'output.txt')
            progress << "$now Starting test $desc.className $desc.name$eol"
            output << "${now} STARTING TEST ${desc.className} ${desc.name}${eol}${eol}"
          }
          onOutput { desc, event ->
            def output = new File(workingDir, 'output.txt')
            output  << event.message
          }
          afterTest { desc, result ->
            def now = new Date().format('yyyy-MM-dd HH:mm:ss.SSS Z')
            def progress = new File(workingDir, 'progress.txt')
            def output = new File(workingDir, 'output.txt')
            progress << "${now} Completed test ${desc.className} ${desc.name} with result: ${result.resultType}${eol}"
            output << "${eol}${now} COMPLETED TEST ${desc.className} ${desc.name} with result: ${result.resultType}${eol}${eol}"
            result.exceptions.each { t ->
              progress << "  EXCEPTION: ${getStackTrace(t)}${eol}"
              output << "${getStackTrace(t)}${eol}"
            }
          }
        }
      }
    }
  }
}

task generateSources {
  dependsOn subprojectBase + 'snappy-spark-core_' + scalaBinaryVersion + ':generateBuildInfo'
  dependsOn subprojectBase + 'snappy-spark-catalyst_' + scalaBinaryVersion + ':generateGrammarSource'
  dependsOn subprojectBase + 'snappy-spark-streaming-flume-sink_' + scalaBinaryVersion + ':generateAvroJava'
}

check.dependsOn subprojects.check

if (rootProject.name == 'snappy-spark') {
  task scalaStyle {
    dependsOn subprojects.scalaStyle
  }
} else {
  scalaStyle.dependsOn subprojects.scalaStyle
}

task cleanAll {
  dependsOn getTasksByName('clean', true).collect { it.path }
}
task buildAll {
  dependsOn getTasksByName('assemble', true).collect { it.path }
  dependsOn getTasksByName('testClasses', true).collect { it.path }
  dependsOn scalaStyle
  mustRunAfter cleanAll
}
